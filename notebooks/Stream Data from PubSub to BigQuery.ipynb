{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ce46cf",
   "metadata": {},
   "source": [
    "# Creating a DataFlow Job to Stream Data into BigQuery\n",
    "\n",
    "Assuming you have data flowing into a Pub/Sub Topic, you are now able to stream this data into BigQuery whenever new messages appear in the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ac5da",
   "metadata": {},
   "source": [
    "## Create a BigQuery Table\n",
    "\n",
    "Use the schema provided in the class resources git repo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "732ac093",
   "metadata": {},
   "source": [
    "## Configure a DataFlow Job\n",
    "\n",
    "__NOTE:__ You must have a BigQuery table already created with the proper schema in order for the DataFlow job to work. The provided OpenSky code publishes messages in JSON format. The JSON format and the BigQuery table schema must match.\n",
    "\n",
    "1. Create a job from a Template named \"Pub/Sub Topic to BigQuery\".\n",
    "\n",
    "2. Give the job the name of your Pub/Sub Topic and Big Query Table.\n",
    "\n",
    "3. For a Temporary location, use a bucket that you have created and specify a temporary folder in the bucket. For example, I have a bucket named _flight-streaming-data_, so I specify the following for a temporary location:\n",
    ">.   gs://flight-streaming-data/temp\n",
    "\n",
    "4. __NOTE:__ It seems that the default machine type (n1-standard) used for DataFlow jobs produces a __RESOURCES_EXHAUSTED__ error. If your job fails to start due to a RESOURCES_EXHAUSTED error, change the machine type to e2-medium by doing the following:\n",
    "    4.1. Open up the _Optional Parameters_ section at the bottom of the job configuration.\n",
    "    4.2. Deselect \"Use default machine type\". The options for various machine types will show below.\n",
    "    4.3. Select the __E2__ series. The default machine type in this series is e2-medium which is sufficient.\n",
    "\n",
    "5. Run the job.\n",
    "    5.1. The job will silently run. It takes a little time for the resources to start up and be ready to process new messages. The boxes in the Job Graph tab of the DataFlow display will have striped green borders. If they have red borders then the job has failed to start up.\n",
    "    5.2. You can see if the resources have started up by looking at the Logs section at the bottom.\n",
    "        5.2.1. Select SHOW next to Logs.\n",
    "        5.2.2. Select WORKER LOGS.\n",
    "        5.2.3. Change the log severity level from Info to Debug.\n",
    "        5.2.4. You should see logs indicating that there is activity in the workers.\n",
    "    5.3. __NOTE:__ The DataFlow job will not do anything until you publish _new_ messages to the Pub/Sub queue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bear",
   "language": "python",
   "name": "bear"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
